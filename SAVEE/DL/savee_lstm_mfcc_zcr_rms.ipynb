{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0fb8f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 480/480 [00:10<00:00, 44.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŒ€ Fold 1 ---------------------------\n",
      "3/3 [==============================] - 1s 17ms/step\n",
      "Fold 1 Accuracy: 0.4583, F1 Score: 0.3753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.25      0.25      0.25        12\n",
      "     disgust       0.43      0.25      0.32        12\n",
      "        fear       0.50      0.25      0.33        12\n",
      "       happy       0.36      0.33      0.35        12\n",
      "     neutral       0.65      0.92      0.76        24\n",
      "         sad       0.14      0.08      0.11        12\n",
      "    surprise       0.42      0.67      0.52        12\n",
      "\n",
      "    accuracy                           0.46        96\n",
      "   macro avg       0.39      0.39      0.38        96\n",
      "weighted avg       0.43      0.46      0.42        96\n",
      "\n",
      "\n",
      "ðŸŒ€ Fold 2 ---------------------------\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Fold 2 Accuracy: 0.4062, F1 Score: 0.3017\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.33      0.25      0.29        12\n",
      "     disgust       0.20      0.17      0.18        12\n",
      "        fear       0.42      0.42      0.42        12\n",
      "       happy       0.18      0.17      0.17        12\n",
      "     neutral       0.59      1.00      0.74        24\n",
      "         sad       0.33      0.08      0.13        12\n",
      "    surprise       0.20      0.17      0.18        12\n",
      "\n",
      "    accuracy                           0.41        96\n",
      "   macro avg       0.32      0.32      0.30        96\n",
      "weighted avg       0.35      0.41      0.36        96\n",
      "\n",
      "\n",
      "ðŸŒ€ Fold 3 ---------------------------\n",
      "3/3 [==============================] - 1s 38ms/step\n",
      "Fold 3 Accuracy: 0.4896, F1 Score: 0.4206\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.60      0.50      0.55        12\n",
      "     disgust       0.36      0.33      0.35        12\n",
      "        fear       0.40      0.17      0.24        12\n",
      "       happy       0.55      0.50      0.52        12\n",
      "     neutral       0.58      0.88      0.70        24\n",
      "         sad       0.17      0.08      0.11        12\n",
      "    surprise       0.41      0.58      0.48        12\n",
      "\n",
      "    accuracy                           0.49        96\n",
      "   macro avg       0.44      0.43      0.42        96\n",
      "weighted avg       0.46      0.49      0.46        96\n",
      "\n",
      "\n",
      "ðŸŒ€ Fold 4 ---------------------------\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027853775C60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 16ms/step\n",
      "Fold 4 Accuracy: 0.4583, F1 Score: 0.4283\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.58      0.58      0.58        12\n",
      "     disgust       0.32      0.50      0.39        12\n",
      "        fear       0.22      0.17      0.19        12\n",
      "       happy       0.67      0.50      0.57        12\n",
      "     neutral       0.48      0.67      0.56        24\n",
      "         sad       0.50      0.17      0.25        12\n",
      "    surprise       0.50      0.42      0.45        12\n",
      "\n",
      "    accuracy                           0.46        96\n",
      "   macro avg       0.47      0.43      0.43        96\n",
      "weighted avg       0.47      0.46      0.44        96\n",
      "\n",
      "\n",
      "ðŸŒ€ Fold 5 ---------------------------\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027840BB6520> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 18ms/step\n",
      "Fold 5 Accuracy: 0.5521, F1 Score: 0.4987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.56      0.42      0.48        12\n",
      "     disgust       0.56      0.42      0.48        12\n",
      "        fear       0.36      0.42      0.38        12\n",
      "       happy       0.50      0.58      0.54        12\n",
      "     neutral       0.71      0.92      0.80        24\n",
      "         sad       0.43      0.25      0.32        12\n",
      "    surprise       0.50      0.50      0.50        12\n",
      "\n",
      "    accuracy                           0.55        96\n",
      "   macro avg       0.52      0.50      0.50        96\n",
      "weighted avg       0.54      0.55      0.54        96\n",
      "\n",
      "\n",
      "ðŸŒŸ Final Cross-Validation Results:\n",
      "Average Accuracy: 0.4729\n",
      "Average F1 Score: 0.4049\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "# ------------------- Load SAVEE -------------------\n",
    "def load_savee_audio(dataset_path):\n",
    "    audio_files, labels = [], []\n",
    "    label_map = {'a': 'angry', 'd': 'disgust', 'f': 'fear', 'h': 'happy', 'n': 'neutral', 'sa': 'sad', 'su': 'surprise'}\n",
    "    for file in os.listdir(dataset_path):\n",
    "        if file.endswith(\".wav\"):\n",
    "            parts = file.split('_')\n",
    "            emotion_code = parts[1][:2] if parts[1][:2] in label_map else parts[1][0]\n",
    "            if emotion_code in label_map:\n",
    "                audio_files.append(os.path.join(dataset_path, file))\n",
    "                labels.append(label_map[emotion_code])\n",
    "    return audio_files, labels\n",
    "\n",
    "# ------------------- Feature Extraction -------------------\n",
    "def extract_mfcc_features(file_path, max_len=100):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    features = np.vstack([mfcc, zcr, rms])  # (43, time_steps)\n",
    "\n",
    "    if features.shape[1] < max_len:\n",
    "        pad_width = max_len - features.shape[1]\n",
    "        features = np.pad(features, pad_width=((0,0),(0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        features = features[:, :max_len]\n",
    "    return features.T  # (time_steps, 43)\n",
    "\n",
    "# ------------------- Load & Preprocess -------------------\n",
    "dataset_path = \"C:/Users/samhi/OneDrive/ë¬¸ì„œ/College/s6/Speech Processing/Endsem/archive/ALL\"\n",
    "audio_files, labels = load_savee_audio(dataset_path)\n",
    "\n",
    "X, y_clean = [], []\n",
    "for file, label in tqdm(zip(audio_files, labels), total=len(audio_files)):\n",
    "    try:\n",
    "        feat = extract_mfcc_features(file)\n",
    "        X.append(feat)\n",
    "        y_clean.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed on {file}: {e}\")\n",
    "\n",
    "X = np.array(X)\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y_clean)\n",
    "num_classes = len(np.unique(y))\n",
    "\n",
    "# ------------------- K-Fold CV Setup -------------------\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "acc_scores, f1_scores = [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "    print(f\"\\n Fold {fold+1} ---------------------------\")\n",
    "    \n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    tf.keras.backend.clear_session()  # Clean previous model from memory\n",
    "\n",
    "    model = Sequential([\n",
    "        LSTM(128, return_sequences=False, input_shape=(X.shape[1], X.shape[2])),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, epochs=40, batch_size=16, validation_data=(X_test, y_test),\n",
    "              callbacks=[EarlyStopping(patience=5, restore_best_weights=True)], verbose=0)\n",
    "\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    acc_scores.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Fold {fold+1} Accuracy: {acc:.4f}, F1 Score: {f1:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# ------------------- Final Results -------------------\n",
    "print(\"\\nFinal Cross-Validation Results:\")\n",
    "print(f\"Average Accuracy: {np.mean(acc_scores):.4f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1_scores):.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
